#!/usr/bin/env python3
"""
Enhanced Bengali Legal RAG with Latest EmbeddingGemma Optimizations
=================================================================
Implements Matryoshka Representation Learning, task prompts, and performance optimizations
based on the latest EmbeddingGemma improvements.
"""

import os
import pandas as pd
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import logging
import time
import torch
import faiss
import pickle
from sentence_transformers import SentenceTransformer
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Environment setup
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["OMP_NUM_THREADS"] = "1"

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnhancedEmbeddingGemmaRAG:
    """Enhanced Bengali Legal RAG with latest EmbeddingGemma optimizations"""
    
    def __init__(self, 
                 data_dir: str = "data/production_bengali_legal_dataset", 
                 confidence_threshold: float = 0.5,
                 embedding_dim: int = 768,
                 use_task_prompts: bool = True):
        """
        Initialize with latest EmbeddingGemma optimizations
        
        Args:
            data_dir: Directory containing training data
            confidence_threshold: Minimum confidence for predictions
            embedding_dim: Embedding dimension (768, 512, 256, or 128 for MRL)
            use_task_prompts: Whether to use task-specific prompts
        """
        logger.info(f"üöÄ Loading Enhanced EmbeddingGemma-300M (dim={embedding_dim})...")
        
        # Use official MRL with truncate_dim and dot product similarity
        if embedding_dim < 768:
            self.model = SentenceTransformer(
                "google/embeddinggemma-300m", 
                truncate_dim=embedding_dim,
                similarity_fn_name="dot"
            )
        else:
            self.model = SentenceTransformer(
                "google/embeddinggemma-300m",
                similarity_fn_name="dot"
            )
        self.confidence_threshold = confidence_threshold
        self.data_dir = Path(data_dir)
        self.embedding_dim = embedding_dim
        self.use_task_prompts = use_task_prompts
        
        # Validate embedding dimension for MRL
        valid_dims = [768, 512, 256, 128]
        if embedding_dim not in valid_dims:
            logger.warning(f"Embedding dim {embedding_dim} not in {valid_dims}, using 768")
            self.embedding_dim = 768
        
        # Performance optimization
        if torch.backends.mps.is_available():
            torch.mps.empty_cache()
        
        # Data storage
        self.train_questions = []
        self.train_tags = []
        self.val_questions = []
        self.val_tags = []
        self.index = None
        
        # Index cache paths
        self.index_path = self.data_dir / f"gemma_index_{embedding_dim}d.faiss"
        self.metadata_path = self.data_dir / f"gemma_metadata_{embedding_dim}d.pkl"
        
        # Load and build index
        self._load_data()
        self._load_or_build_index()
    
    def _get_prompt_name(self, task_type: str = "query") -> str:
        """Get official EmbeddingGemma prompt name for task type"""
        if not self.use_task_prompts:
            return None
            
        # Use official Classification prompt for both queries and documents
        # as this is a classification task
        return "Classification"
    
    def _load_data(self):
        """Load training and validation data with enhanced processing"""
        logger.info("Loading and preprocessing data...")
        
        csv_files = list(self.data_dir.glob("namjari_*.csv"))
        
        for csv_file in csv_files:
            tag = csv_file.stem.replace("namjari_", "")
            df = pd.read_csv(csv_file)
            questions = df['question'].tolist()
            
            # Enhanced preprocessing
            processed_questions = []
            for question in questions:
                # Clean and normalize text
                cleaned = question.strip()
                if len(cleaned) > 3:  # Filter very short questions
                    processed_questions.append(cleaned)
            
            # Split into train/validation
            val_questions = processed_questions[-10:]
            train_questions = processed_questions[:-10]
            
            self.val_questions.extend(val_questions)
            self.val_tags.extend([tag] * len(val_questions))
            
            self.train_questions.extend(train_questions)
            self.train_tags.extend([tag] * len(train_questions))
        
        logger.info(f"‚úÖ Loaded {len(self.train_questions)} training and {len(self.val_questions)} validation samples")
    
    def _load_or_build_index(self):
        """Load existing index or build new one with caching"""
        if self.index_path.exists() and self.metadata_path.exists():
            logger.info(f"üìÇ Loading cached FAISS index from {self.index_path}")
            try:
                # Load index
                self.index = faiss.read_index(str(self.index_path))
                
                # Load metadata
                with open(self.metadata_path, 'rb') as f:
                    metadata = pickle.load(f)
                
                # Verify metadata matches current setup
                if (metadata.get('embedding_dim') == self.embedding_dim and 
                    metadata.get('use_task_prompts') == self.use_task_prompts and
                    metadata.get('num_samples') == len(self.train_questions)):
                    
                    logger.info(f"‚úÖ Loaded cached index: {self.index.ntotal} vectors, {self.index.d} dimensions")
                    return
                else:
                    logger.info("‚ö†Ô∏è Cached index metadata mismatch, rebuilding...")
            except Exception as e:
                logger.info(f"‚ö†Ô∏è Failed to load cached index: {e}, rebuilding...")
        
        # Build new index
        self._build_enhanced_index()
        
        # Save index and metadata
        logger.info(f"üíæ Saving FAISS index to {self.index_path}")
        faiss.write_index(self.index, str(self.index_path))
        
        metadata = {
            'embedding_dim': self.embedding_dim,
            'use_task_prompts': self.use_task_prompts,
            'num_samples': len(self.train_questions),
            'model_name': 'google/embeddinggemma-300m',
            'created_at': time.time()
        }
        
        with open(self.metadata_path, 'wb') as f:
            pickle.dump(metadata, f)
        
        logger.info(f"‚úÖ Index and metadata saved!")

    def _build_enhanced_index(self):
        """Build enhanced FAISS index with official EmbeddingGemma optimizations"""
        logger.info("Building enhanced FAISS index with official optimizations...")
        
        # Generate embeddings with official Classification prompt
        start_time = time.time()
        prompt_name = self._get_prompt_name("document")
        
        if prompt_name:
            embeddings = self.model.encode(
                self.train_questions, 
                prompt_name=prompt_name,
                batch_size=32, 
                show_progress_bar=True,
                convert_to_numpy=True,
                normalize_embeddings=True  # Official recommendation
            )
        else:
            embeddings = self.model.encode(
                self.train_questions, 
                batch_size=32, 
                show_progress_bar=True,
                convert_to_numpy=True,
                normalize_embeddings=True
            )
        embedding_time = time.time() - start_time
        
        # No manual truncation needed - handled by truncate_dim in model init
        
        # Build optimized FAISS index
        start_time = time.time()
        self.index = faiss.IndexFlatIP(embeddings.shape[1])
        self.index.add(embeddings.astype('float32'))
        index_time = time.time() - start_time
        
        logger.info(f"‚úÖ Enhanced FAISS IndexFlatIP ready:")
        logger.info(f"   - Vectors: {self.index.ntotal}")
        logger.info(f"   - Dimensions: {embeddings.shape[1]}")
        logger.info(f"   - Embedding time: {embedding_time:.2f}s")
        logger.info(f"   - Index build time: {index_time:.2f}s")
    
    def query(self, question: str, k: int = 3) -> Dict:
        """Enhanced query with official EmbeddingGemma optimizations"""
        # Generate embedding with official Classification prompt
        start_time = time.time()
        prompt_name = self._get_prompt_name("query")
        
        if prompt_name:
            query_embedding = self.model.encode(
                [question], 
                prompt_name=prompt_name,
                show_progress_bar=False,
                convert_to_numpy=True,
                normalize_embeddings=True
            )
        else:
            query_embedding = self.model.encode(
                [question], 
                show_progress_bar=False,
                convert_to_numpy=True,
                normalize_embeddings=True
            )
        
        # No manual processing needed - handled by model
        
        # Search
        scores, indices = self.index.search(query_embedding.astype('float32'), k)
        query_time = time.time() - start_time
        
        # Process results
        results = []
        for i in range(k):
            results.append({
                'question': self.train_questions[indices[0][i]],
                'category': self.train_tags[indices[0][i]],
                'confidence': float(scores[0][i])
            })
        
        primary_prediction = results[0]['category'] if results[0]['confidence'] >= self.confidence_threshold else 'uncertain'
        
        return {
            'query': question,
            'prediction': primary_prediction,
            'confidence': results[0]['confidence'],
            'query_time': query_time,
            'similar_questions': results
        }
    
    def classify(self, query: str, top_k: int = 5) -> Dict:
        """
        Classify Bengali legal query using semantic search
        
        Args:
            query: Bengali text to classify
            top_k: Number of similar examples to consider
            
        Returns:
            Classification result with confidence score
        """
        result = self.query(query, k=top_k)
        
        return {
            'query': query,
            'predicted_tag': result['prediction'],
            'confidence': result['confidence'],
            'is_confident': result['confidence'] >= self.confidence_threshold,
            'similar_questions': result['similar_questions']
        }
    
    def evaluate_validation_set(self) -> Dict:
        """Evaluate system on validation set and return metrics"""
        logger.info("üß™ Evaluating on validation set...")
        
        predictions = []
        true_labels = []
        confidences = []
        
        for question, true_tag in zip(self.val_questions, self.val_tags):
            result = self.classify(question)
            predictions.append(result['predicted_tag'])
            true_labels.append(true_tag)
            confidences.append(result['confidence'])
        
        # Calculate accuracy
        accuracy = accuracy_score(true_labels, predictions)
        avg_confidence = np.mean(confidences)
        
        # Count confident predictions
        confident_predictions = sum(1 for c in confidences if c >= self.confidence_threshold)
        
        return {
            'accuracy': accuracy,
            'average_confidence': avg_confidence,
            'confident_predictions': confident_predictions,
            'total_samples': len(self.val_questions),
            'predictions': predictions,
            'true_labels': true_labels,
            'confidences': confidences
        }
    
    def create_confusion_matrix(self, save_path: str = None) -> Tuple[np.ndarray, List[str]]:
        """Create and save confusion matrix"""
        logger.info("üìä Generating confusion matrix...")
        
        # Get evaluation results
        eval_results = self.evaluate_validation_set()
        
        # Get unique labels
        all_labels = sorted(list(set(eval_results['true_labels'] + eval_results['predictions'])))
        
        # Create confusion matrix
        cm = confusion_matrix(eval_results['true_labels'], eval_results['predictions'], labels=all_labels)
        
        # Create visualization
        plt.figure(figsize=(12, 10))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=[label.replace('namjari_', '') for label in all_labels],
                   yticklabels=[label.replace('namjari_', '') for label in all_labels])
        
        plt.title(f'Bengali Legal RAG Confusion Matrix\nAccuracy: {eval_results["accuracy"]:.1%} | Confidence: {eval_results["average_confidence"]:.3f}')
        plt.xlabel('Predicted Category')
        plt.ylabel('True Category')
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        # Save if path provided
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"üìä Confusion matrix saved to: {save_path}")
        
        plt.show()
        
        return cm, all_labels

    def get_stats(self) -> Dict:
        """Get system statistics"""
        return {
            'model': 'google/embeddinggemma-300m',
            'faiss_index': 'IndexFlatIP',
            'training_samples': len(self.train_questions),
            'validation_samples': len(self.val_questions),
            'total_tags': len(set(self.train_tags)),
            'confidence_threshold': self.confidence_threshold,
            'device': str(self.model.device)
        }

def main():
    """Main demo function"""
    print("üöÄ Enhanced Bengali Legal RAG System")
    print("=" * 50)
    print("üìã EmbeddingGemma-300M + FAISS IndexFlatIP")
    print("=" * 50)
    
    # Initialize system
    rag = EnhancedEmbeddingGemmaRAG()
    
    # Test sample queries  
    print(f"\nüìù Testing Sample Queries:")
    print("-" * 40)
    
    test_queries = [
        "‡¶Ü‡¶Æ‡¶ø ‡¶®‡¶æ‡¶Æ‡¶ú‡¶æ‡¶∞‡¶ø ‡¶ï‡¶∞‡¶§‡ßá ‡¶ï‡¶§ ‡¶ü‡¶æ‡¶ï‡¶æ ‡¶≤‡¶æ‡¶ó‡¶¨‡ßá?",
        "‡¶®‡¶æ‡¶Æ‡¶ú‡¶æ‡¶∞‡¶ø‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ï‡¶ø ‡¶ï‡¶ø ‡¶ï‡¶æ‡¶ó‡¶ú‡¶™‡¶§‡ßç‡¶∞ ‡¶≤‡¶æ‡¶ó‡ßá?",
        "‡¶Ü‡¶Æ‡¶ø ‡¶®‡¶æ‡¶Æ‡¶ú‡¶æ‡¶∞‡¶ø ‡¶Ü‡¶¨‡ßá‡¶¶‡¶® ‡¶ï‡¶ø‡¶≠‡¶æ‡¶¨‡ßá ‡¶ï‡¶∞‡¶¨‡ßã?"
    ]
    
    for query in test_queries:
        result = rag.classify(query)
        status = "‚úÖ" if result['is_confident'] else "‚ö†Ô∏è"
        
        print(f"{status} {query}")
        print(f"   ‚Üí {result['predicted_tag'].replace('namjari_', '')} (confidence: {result['confidence']:.3f})")
    
    # Show system stats
    stats = rag.get_stats()
    print(f"\nüìä System Statistics:")
    print(f"  Model: {stats['model']}")
    print(f"  FAISS Index: {stats['faiss_index']}")
    print(f"  Training Samples: {stats['training_samples']}")
    print(f"  Validation Samples: {stats['validation_samples']}")
    print(f"  Tags: {stats['total_tags']}")
    print(f"  Threshold: {stats['confidence_threshold']}")
    
    print(f"\n‚úÖ System ready!")
    
    return rag

def main():
    """Main demo function"""
    print("üöÄ Enhanced Bengali Legal RAG System")
    print("=" * 50)
    print("üìã EmbeddingGemma-300M + FAISS IndexFlatIP")
    print("=" * 50)
    
    # Initialize system
    rag = EnhancedEmbeddingGemmaRAG()
    
    # Test sample queries  
    print(f"\nüìù Testing Sample Queries:")
    print("-" * 40)
    
    test_queries = [
        "‡¶Ü‡¶Æ‡¶ø ‡¶®‡¶æ‡¶Æ‡¶ú‡¶æ‡¶∞‡¶ø ‡¶ï‡¶∞‡¶§‡ßá ‡¶ï‡¶§ ‡¶ü‡¶æ‡¶ï‡¶æ ‡¶≤‡¶æ‡¶ó‡¶¨‡ßá?",
        "‡¶®‡¶æ‡¶Æ‡¶ú‡¶æ‡¶∞‡¶ø‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ï‡¶ø ‡¶ï‡¶ø ‡¶ï‡¶æ‡¶ó‡¶ú‡¶™‡¶§‡ßç‡¶∞ ‡¶≤‡¶æ‡¶ó‡ßá?",
        "‡¶Ü‡¶Æ‡¶ø ‡¶®‡¶æ‡¶Æ‡¶ú‡¶æ‡¶∞‡¶ø ‡¶Ü‡¶¨‡ßá‡¶¶‡¶® ‡¶ï‡¶ø‡¶≠‡¶æ‡¶¨‡ßá ‡¶ï‡¶∞‡¶¨‡ßã?"
    ]
    
    for query in test_queries:
        result = rag.classify(query)
        status = "‚úÖ" if result['is_confident'] else "‚ö†Ô∏è"
        
        print(f"{status} {query}")
        print(f"   ‚Üí {result['predicted_tag'].replace('namjari_', '')} (confidence: {result['confidence']:.3f})")
    
    # Show system stats
    stats = rag.get_stats()
    print(f"\nüìä System Statistics:")
    print(f"  Model: {stats['model']}")
    print(f"  FAISS Index: {stats['faiss_index']}")
    print(f"  Training Samples: {stats['training_samples']}")
    print(f"  Validation Samples: {stats['validation_samples']}")
    print(f"  Tags: {stats['total_tags']}")
    print(f"  Threshold: {stats['confidence_threshold']}")
    
    # Generate confusion matrix
    print(f"\nüé® Generating confusion matrix...")
    cm_path = "bengali_legal_confusion_matrix.png"
    cm, labels = rag.create_confusion_matrix(save_path=cm_path)
    
    # Evaluate system
    eval_results = rag.evaluate_validation_set()
    print(f"\nüéØ VALIDATION RESULTS:")
    print(f"   üìä Accuracy: {eval_results['accuracy']:.1%}")
    print(f"   üíé Confidence: {eval_results['average_confidence']:.3f}")
    print(f"   üìà Confident Predictions: {eval_results['confident_predictions']}/{eval_results['total_samples']}")
    
    print(f"\n‚úÖ System ready!")
    print(f"üìä Confusion matrix saved to: {cm_path}")
    
    return rag

if __name__ == "__main__":
    main()
